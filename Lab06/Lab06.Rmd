---
title: "Lab 06 - Text Mining"
author: "Qiushi"
date: "`r Sys.Date()`"
output: github_document
always_allow_html: true
---
```{r install-libraries}
library(tidytext)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(forcats)
```


## Step 1. Read in the data
First download and then read in data.
```{r read-data, cache = TRUE}
if (!file.exists("mtsamples.csv"))
  download.file(
    url = "https://raw.githubusercontent.com/USCbiostats/data-science-data/master/00_mtsamples/mtsamples.csv",
    destfile = "mtsamples.csv",
    method   = "libcurl",
    timeout  = 60
    )
mts <- read.csv("mtsamples.csv")
# str(mts)
mts <- as_tibble(mts)
str(mts)
```

## Question 1: What specialties do we have?
We can use count() from dplyr to figure out how many different catagories do we have?

```{r medical-specialities}
specialties <- 
  mts %>%
  count(medical_specialty)
specialties %>%
  arrange(desc(n)) %>%
  knitr::kable()
```
There are `r nrow(specialties)` medical specialities. 


Are these catagories related? overlapping? evenly distributed?
```{r barplot-of-specialty-counts}
specialties %>%
  top_n(10) %>%
  ggplot(aes(x = n,y = fct_reorder(medical_specialty, n))) +
  geom_col()
```

The distribution is not at all uniform.

## Question 2 Visualize the top 30 frequent words in the transcription column
Tokenize the the words in the transcription column
Count the number of times each token appears
Visualize the top 20 most frequent words

```{r token-transcription, cache = TRUE}
mts %>%
  unnest_tokens(word, transcription) %>%
  # anti_join(stop_words, by = c("word")) %>%
  count(word, sort = TRUE) %>%
  top_n(20, n) %>%
  ggplot(aes(n, fct_reorder(word, n))) +
    geom_col()

```

There are a lot of stopwords here, nons-specific to medical text.
We do see "patient".

## Question 3
Redo visualization but remove stopwords before
Bonus points if you remove numbers as well
```{r token-transcription-no-stopâ€”word, cache = TRUE}
mts %>%
  unnest_tokens(word, transcription) %>%
  count(word, sort = TRUE) %>%
  anti_join(stop_words, by = c("word")) %>%
  # use regular expression to filter out numbers
  filter(!grepl(pattern = "^[0-9]+$", x = word)) %>%
  top_n(20, n) %>%
  ggplot(aes(n, fct_reorder(word, n))) +
    geom_col()

```

Removing the stopwords and numbers gives us a much better idea of what the text is about.

## Question 4
Repeat question 2, but this time tokenize into bi-grams. How does the result change if you look at tri-grams?

```{r bigrams-transcription, cache = TRUE}
mts %>%
  unnest_ngrams(bigram, transcription, n = 2) %>%
  count(bigram, sort = TRUE) %>%
  top_n(20, n) %>%
  ggplot(aes(n, fct_reorder(bigram, n))) +
    geom_col()

```

Top 20 trigrams seemed to return a few more medical word group than bigrams.

## Question 5
Using the results you got from questions 4. Pick a word and count the words that appears after and before it.

```{r bigrams-transcription-nextword, cache = TRUE}
ptbigram <- 
  mts %>%
  unnest_ngrams(bigram, transcription, n = 2) %>%
  separate(bigram, into = c("word1", "word2"), sep = " ") %>%
  select(word1, word2) %>%
  filter(word1 == "patient" | word2 == "patient")

```

Words appearing before patients:
```{r before-patient, cache = TRUE}
ptbigram %>%
  filter(word1 == "patient") %>%
  count(word2, sort = TRUE) %>%
  anti_join(stop_words, by = c("word2" = "word")) %>%
  top_n(10)  %>% 
  knitr::kable()

```



# Question 6
Which words are most used in each of the specialties. you can use group_by() and top_n() from dplyr to have the calculations be done within each specialty. Remember to remove stopwords. How about the most 5 used words?

```{r top5-words-per-specialty, cache = TRUE}
mts %>%
  unnest_tokens(word, transcription) %>%
  group_by(medical_specialty) %>%
  count(word, sort = TRUE) %>%
  filter((!word %in% stop_words$word) & (!grepl(pattern = "^[0-9]+$", x = word))) %>%
  top_n(5, n) %>%
  arrange(medical_specialty, desc(n)) %>%
  knitr::kable()

```

